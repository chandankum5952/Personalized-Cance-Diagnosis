{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data\n## Reading Gene and Variation Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/msk-redefining-cancer-treatment/training_variants.zip\")\nprint(\"No. of feature\" , data.shape[1])\nprint(\"No. of datapts\" , data.shape[0])\nprint(\"first few row \" , data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading text data\n\ndata_text = pd.read_csv(\"/kaggle/input/msk-redefining-cancer-treatment/training_text.zip\" , sep = \"\\|\\|\" ,engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1 )\nprint(\"No. of Datapoints\" , data_text.shape[0])\nprint(\"No. of features\" , data_text.shape[1])\nprint(\"Features\" , data_text.columns.values)\ndata_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing of Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n\n\ndef nlp_preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        data_text[column][index] = string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text processing stage.\nstart_time = time.clock()\n# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas?answertab=active#tab-top\nfor index, row in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        nlp_preprocessing(row['TEXT'], index, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\nprint(data_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging both data\n\nresult = pd.merge(data , data_text , on = \"ID\" , how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking how many null values are there\n# https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\nresult.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we saw that a large amount of data having null value so we put Text = Gene + Variation\n\nresult.loc[result[\"TEXT\"].isnull() , \"TEXT\"] = result[\"Gene\"] + ' ' + result[\"Variation\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result[\"ID\"] == 5667]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test and CV Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = result[\"Class\"].values\n\n# https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe\n# del result[\"Class\"] \nresult.Gene = result.Gene.str.replace(\"\\s+\" , \"_\")\nresult.Variation = result.Variation.str.replace(\"\\s+\" , \"_\")\n\n# Now split the data into 3 dataset , train, test and CV\n\nX_train , test_df , y_train , y_test = train_test_split(result , y_true , stratify = y_true , test_size = 0.2)\ntrain_df , cv_df , y_train ,  y_cv = train_test_split(X_train , y_train , stratify = y_train , test_size =0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we distributed tha data into train , test and CV (64:16:20 ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data {} and percent : {}\".format(train_df.shape[0] , (train_df.shape[0]/result.shape[0])))\nprint(\" Number of datapts in cv {} and percent of data : {} \".format(cv_df.shape[0] , (cv_df.shape[0]) / result.shape[0] ))\nprint(\" No. of datapts in test {} & percent {}\".format(test_df.shape[0] , (test_df.shape[0] /result.shape[0]) ))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of class label of y classes(output) in train , test and CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_class_distb = train_df[\"Class\"].value_counts().sort_index()\ncv_class_distb = cv_df[\"Class\"].value_counts().sort_index()\ntest_class_distb = test_df[\"Class\"].value_counts().sort_index()\n\n#mycolors = \"Pastel1\"\nmycolors = ['r', 'g', 'b', 'k', 'y', 'm', 'c' ,'k' , 'w']\n\ntrain_class_distb.plot(kind = \"bar\" , color = mycolors)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Datapoint per class\")\nplt.title(\"Distribution of yi in train data\")\nplt.grid()\nplt.show()\n\n# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\n\nsorted_yi = np.argsort(-train_class_distb.values)\n\nfor i in sorted_yi:\n    print(\"Number of data points in class \" , i+1 , ':' , train_class_distb.values[i] , \"(\" , np.round((train_class_distb.values[i] / train_df.shape[0] * 100) ,3) ,\"%)\")\n    \n\nprint(\"=\" *88) \n#mycolors = \"Pastel1\"\nmycolors = ['r', 'g', 'b', 'k', 'y', 'm', 'c' ,'k' , 'w']\n\ntrain_class_distb.plot(kind = \"bar\" , color = mycolors)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Datapoint per class\")\nplt.title(\"Distribution of yi in train data\")\nplt.grid()\nplt.show()\n\n# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\n\nsorted_yi = np.argsort(-cv_class_distb)\n\nfor  i in sorted_yi:\n    print(\"No. of datapts in class \" , i+1 , \":\" , cv_class_distb.values[i] , \"(\" , np.round((cv_class_distb.values[i] / cv_df.shape[0] *100) ,3) , \"%)\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Using Random Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n\ncv_predicted_y = np.zeros((cv_data_len , 9))\n\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs / sum(sum(rand_probs)))[0]) \n    \nprint(\"log loss of random classifier\" , log_loss(y_cv , cv_predicted_y , eps = 1e-15)) \n\n#Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = np.random.rand(1,9)\nprint(w)\ns = sum(w)\nprint(s)\ns1 = sum(sum(w))\nprint(s1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"apple = {\"A\" : 2 , \"B\" : 2 , \"c\" : 3} \napple.items()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis : "},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for response coding with Laplace smoothing.\n# alpha : used for laplace smoothing\n# feature: ['gene', 'variation']\n# df: ['train_df', 'test_df', 'cv_df']\n# algorithm\n# ----------\n# Consider all unique values and the number of occurances of given feature in train data dataframe\n# build a vector (1*9) , the first element = (number of times it occured in class1 + 10*alpha / number of time it occurred in total data+90*alpha)\n# gv_dict is like a look up table, for every gene it store a (1*9) representation of it\n# for a value of feature in df:\n# if it is in train data:\n# we add the vector that was stored in 'gv_dict' look up table to 'gv_fea'\n# if it is not there is train:\n# we add [1/9, 1/9, 1/9, 1/9,1/9, 1/9, 1/9, 1/9, 1/9] to 'gv_fea'\n# return 'gv_fea'\n# ----------------------\n\n# get_gv_fea_dict: Get Gene varaition Feature Dict\ndef get_gv_fea_dict(alpha, feature, df):\n    # value_count: it contains a dict like\n    # print(train_df['Gene'].value_counts())\n    # output:\n    #        {BRCA1      174\n    #         TP53       106\n    #         EGFR        86\n    #         BRCA2       75\n    #         PTEN        69\n    #         KIT         61\n    #         BRAF        60\n    #         ERBB2       47\n    #         PDGFRA      46\n    #         ...}\n    # print(train_df['Variation'].value_counts())\n    # output:\n    # {\n    # Truncating_Mutations                     63\n    # Deletion                                 43\n    # Amplification                            43\n    # Fusions                                  22\n    # Overexpression                            3\n    # E17K                                      3\n    # Q61L                                      3\n    # S222D                                     2\n    # P130S                                     2\n    # ...\n    # }\n    value_count = train_df[feature].value_counts()\n    \n    # gv_dict : Gene Variation Dict, which contains the probability array for each gene/variation\n    gv_dict = dict()\n    \n    # denominator will contain the number of time that particular feature occured in whole data\n    for i, denominator in value_count.items():\n        # vec will contain (p(yi==1/Gi) probability of gene/variation belongs to perticular class\n        # vec is 9 diamensional vector\n        vec = []\n        for k in range(1,10):\n            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n            #         ID   Gene             Variation  Class  \n            # 2470  2470  BRCA1                S1715C      1   \n            # 2486  2486  BRCA1                S1841R      1   \n            # 2614  2614  BRCA1                   M1R      1   \n            # 2432  2432  BRCA1                L1657P      1   \n            # 2567  2567  BRCA1                T1685A      1   \n            # 2583  2583  BRCA1                E1660G      1   \n            # 2634  2634  BRCA1                W1718L      1   \n            # cls_cnt.shape[0] will return the number of rows\n\n            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n            \n            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n\n        # we are adding the gene/variation to the dict as key and vec as value\n        gv_dict[i]=vec\n    return gv_dict\n\n# Get Gene variation feature\ndef get_gv_feature(alpha, feature, df):\n    # print(gv_dict)\n    #     {'BRCA1': [0.20075757575757575, 0.03787878787878788, 0.068181818181818177, 0.13636363636363635, 0.25, 0.19318181818181818, 0.03787878787878788, 0.03787878787878788, 0.03787878787878788], \n    #      'TP53': [0.32142857142857145, 0.061224489795918366, 0.061224489795918366, 0.27040816326530615, 0.061224489795918366, 0.066326530612244902, 0.051020408163265307, 0.051020408163265307, 0.056122448979591837], \n    #      'EGFR': [0.056818181818181816, 0.21590909090909091, 0.0625, 0.068181818181818177, 0.068181818181818177, 0.0625, 0.34659090909090912, 0.0625, 0.056818181818181816], \n    #      'BRCA2': [0.13333333333333333, 0.060606060606060608, 0.060606060606060608, 0.078787878787878782, 0.1393939393939394, 0.34545454545454546, 0.060606060606060608, 0.060606060606060608, 0.060606060606060608], \n    #      'PTEN': [0.069182389937106917, 0.062893081761006289, 0.069182389937106917, 0.46540880503144655, 0.075471698113207544, 0.062893081761006289, 0.069182389937106917, 0.062893081761006289, 0.062893081761006289], \n    #      'KIT': [0.066225165562913912, 0.25165562913907286, 0.072847682119205295, 0.072847682119205295, 0.066225165562913912, 0.066225165562913912, 0.27152317880794702, 0.066225165562913912, 0.066225165562913912], \n    #      'BRAF': [0.066666666666666666, 0.17999999999999999, 0.073333333333333334, 0.073333333333333334, 0.093333333333333338, 0.080000000000000002, 0.29999999999999999, 0.066666666666666666, 0.066666666666666666],\n    #      ...\n    #     }\n    gv_dict = get_gv_fea_dict(alpha, feature, df)\n    # value_count is similar in get_gv_fea_dict\n    value_count = train_df[feature].value_counts()\n    \n    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n    gv_fea = []\n    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n    for index, row in df.iterrows():\n        if row[feature] in dict(value_count).keys():\n            gv_fea.append(gv_dict[row[feature]])\n        else:\n            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n    return gv_fea","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis On Gene Feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q1) Which type of feature is Gene ?"},{"metadata":{},"cell_type":"markdown","source":"#### Ans- It is a categorical feature."},{"metadata":{},"cell_type":"markdown","source":"### Q2) How many categories are there and how they are distributed ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Gene is a categorical feature , \nunique_genes = train_df[\"Gene\"].value_counts()\nprint(\"Unique no. of genes :\" , unique_genes.shape[0])\nprint(unique_genes.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ans-  There are 231 categories in the Gene feature, and they are distributed as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"s =sum(unique_genes.values)\nh = unique_genes.values/s\n\nplt.plot(h, label = \"Histogram of Genes\")\nplt.xlabel(\"Index of a Gene\")\nplt.ylabel(\"Number of Occurences\")\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nplt.plot(c , label = \"Cumulative distribution of Genes\")\nplt.xlabel(\"Index of Gene\")\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How to featurize gene feature?"},{"metadata":{},"cell_type":"markdown","source":"#### Ans- There are two ways to featurize this \na) One hot encoding(It is similar tobinary method) <br>\nb) Response coding (It is probability based method with Laplace Smoothing) <br>\n\nWe choose featurization model as per the ML model we choos when we use Decision Tree then we need less no. of dimension (Response coding),\nand when Log. Regression then we choose One hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"#response-coding of the Gene feature\n# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n# test gene feature\ntest_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n# cross validation gene feature\ncv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:\", train_gene_feature_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot Encoding\ngene_vectorizer = CountVectorizer()\ntrain_gene_ohe = gene_vectorizer.fit_transform(train_df[\"Gene\"])\ntest_gene_ohe = gene_vectorizer.transform(test_df[\"Gene\"])\ncv_gene_ohe = gene_vectorizer.transform(cv_df[\"Gene\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Gene\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train gene feature is converted into one hot encoding feature\" , train_gene_ohe.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q4) How Good the Gene Feature is ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this we will apply a model with input - Gene Feature , Output - Class label ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 1)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_ohe, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_ohe, y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_ohe)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_gene_ohe, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_gene_ohe, y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_gene_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Q5.) Is the Gene feature across all the datasets (Train , CV & Test)?\nAns- Yes as we see the value of log loss value we can say that it has more impact."},{"metadata":{},"cell_type":"markdown","source":"<h3>3.2.2 Univariate Analysis on Variation Feature</h3>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px;\"> <b>Q7.</b> Variation, What type of feature is it ?</p>\n<p style=\"font-size:16px;\"><b>Ans.</b> Variation is a categorical variable </p>\n<p style=\"font-size:18px;\"> <b>Q8.</b> How many categories are there?</p>"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_variations = train_df['Variation'].value_counts()\nprint('Number of Unique Variations :', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Ans: There are\", unique_variations.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows\",)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sum(unique_variations.values);\nh = unique_variations.values/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nprint(c)\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px;\"> <b>Q9.</b> How to featurize this Variation feature ?</p>\n\n<p style=\"font-size:16px;\"><b>Ans.</b>There are two ways we can featurize this variable\ncheck out this video: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/\n<ol><li>One hot Encoding</li><li>Response coding</li></ol></p>\n<p> We will be using both these methods to featurize the Variation Feature </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n# test gene feature\ntest_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n# cross validation gene feature\ncv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_variation_feature_responseCoding is a converted feature using the response coding method. The shape of Variation feature:\", train_variation_feature_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of variation feature.\nvariation_vectorizer = CountVectorizer()\ntrain_variation_ohe = variation_vectorizer.fit_transform(train_df['Variation'])\ntest_variation_ohe = variation_vectorizer.transform(test_df['Variation'])\ncv_variation_ohe = variation_vectorizer.transform(cv_df['Variation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_variation_feature_onehotEncoded is converted feature using the onne-hot encoding method. The shape of Variation feature:\", train_variation_ohe.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px;\"> <b>Q10.</b> How good is this Variation feature  in predicting y_i?</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 1)]\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_ohe, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_ohe, y_train)\n    predict_y = sig_clf.predict_proba(cv_variation_ohe)\n    \n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_ohe, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_ohe, y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_variation_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_ohe)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:18px;\"> <b>Q11.</b> Is the Variation feature stable across all the data sets (Test, Train, Cross validation)?</p>\n<p style=\"font-size:16px;\"> <b>Ans.</b> Not sure! But lets be very sure using the below analysis. </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Q12. How many data points are covered by total \", unique_variations.shape[0], \" genes in test and cross validation data sets?\")\ntest_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\ncv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\nprint('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis oon Text Feature "},{"metadata":{},"cell_type":"markdown","source":"Q1) How many unique words are present in train data?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cls_text is a data frame\n# for every row in data fram consider the 'TEXT'\n# split the words by space\n# make a dict with those words\n# increment its count whenever we see that word\n\ndef extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#https://stackoverflow.com/a/1602964\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building a CountVectorizer with all the words that occured minimum 3 times in train data and I take top 3000 words\n# who have high tfidf value\ntext_vectorizer = TfidfVectorizer(min_df=3 , max_features = 3000)\ntrain_text_feature_tfidf = text_vectorizer.fit_transform(train_df['TEXT'])\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_tfidf.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_tfidf.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_list = []\n# dict_list =[] contains 9 dictoinaries each corresponds to a class\nfor i in range(1,10):\n    cls_text = train_df[train_df['Class']==i]\n    # build a word dict based on the words in that class\n    dict_list.append(extract_dictionary_paddle(cls_text))\n    # append it to dict_list\n\n# dict_list[i] is build on i'th  class text data\n# total_dict is buid on whole training text data\ntotal_dict = extract_dictionary_paddle(train_df)\n\n\nconfuse_array = []\nfor i in train_text_features:\n    ratios = []\n    max_val = -1\n    for j in range(0,9):\n        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n    confuse_array.append(ratios)\nconfuse_array = np.array(confuse_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#response coding of text features\ntrain_text_feature_responseCoding  = get_text_responsecoding(train_df)\ntest_text_feature_responseCoding  = get_text_responsecoding(test_df)\ncv_text_feature_responseCoding  = get_text_responsecoding(cv_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/16202486\n# we convert each row values such that they sum to 1  \ntrain_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\ntest_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\ncv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# don't forget to normalize every feature\ntrain_text_feature_tfidf = normalize(train_text_feature_tfidf, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_tfidf = text_vectorizer.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_tfidf = normalize(test_text_feature_tfidf, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_tfidf = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\ncv_text_feature_tfidf = normalize(cv_text_feature_tfidf, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/a/2258273/4084039\nsorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_fea_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words for a given frequency.\nprint(Counter(sorted_text_occur))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a Logistic regression+Calibration model using text features whicha re on-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_tfidf, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_tfidf, y_train)\n    predict_y = sig_clf.predict_proba(cv_text_feature_tfidf)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_tfidf, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_tfidf, y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_text_feature_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_intersec_text(df):\n    df_text_vec = TfidfVectorizer(min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names()\n\n    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) & set(df_text_features))\n    return len1,len2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len1,len2 = get_intersec_text(test_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of test data appeared in train data\")\nlen1,len2 = get_intersec_text(cv_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>4. Machine Learning Models</h1><h1>4. Machine Learning Models</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data preparation for ML models.\n\n#Misc. functionns for ML models\n\n\ndef predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(test_x)\n    return log_loss(test_y, sig_clf_probs, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = TfidfVectorizer()\n    var_count_vec = TfidfVectorizer()\n    text_count_vec = TfidfVectorizer(min_df=3)\n    \n    gene_vec = gene_count_vec.fit(train_df['Gene'])\n    var_vec  = var_count_vec.fit(train_df['Variation'])\n    text_vec = text_count_vec.fit(train_df['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_count_vec.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:24px;text-align:Center\"> <b>Stacking the three types of features </b><p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging gene, variance and text features\n\n# building train, test and cross validation data sets\n# a = [[1, 2], \n#      [3, 4]]\n# b = [[4, 5], \n#      [6, 7]]\n# hstack(a, b) = [[1, 2, 4, 5],\n#                [ 3, 4, 6, 7]]\n\ntrain_gene_var_onehotCoding = hstack((train_gene_ohe,train_variation_ohe))\ntest_gene_var_onehotCoding = hstack((test_gene_ohe,test_variation_ohe))\ncv_gene_var_onehotCoding = hstack((cv_gene_ohe,cv_variation_ohe))\n\ntrain_x_tfidf = hstack((train_gene_var_onehotCoding, train_text_feature_tfidf)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_tfidf = hstack((test_gene_var_onehotCoding, test_text_feature_tfidf)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_tfidf = hstack((cv_gene_var_onehotCoding, cv_text_feature_tfidf)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n\n\ntrain_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"tfidf features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_tfidf.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_tfidf.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Response encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find more about Multinomial Naive base function here http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n# -------------------------\n# default paramters\n# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n\n# some of methods of MultinomialNB()\n# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n# predict(X)\tPerform classification on an array of test vectors X.\n# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n# -----------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n# -----------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n# ----------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n# -----------------------\n\n\nalpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000  , 10000 , 100000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>4.1.1.2. Testing the model with best hyper paramters</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"a= 2\na","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4.2. K Nearest Neighbour Classification</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.2.1. Hyper parameter tuning</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n# -------------------------\n# default parameter\n# KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, \n# metric=’minkowski’, metric_params=None, n_jobs=1, **kwargs)\n\n# methods of\n# fit(X, y) : Fit the model using X as training data and y as target values\n# predict(X):Predict the class labels for the provided data\n# predict_proba(X):Return probability estimates for the test data X.\n#-------------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n#-------------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\n\nalpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_x_responseCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_responseCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.2.2. Testing the model with best hyper paramters</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n# -------------------------\n# default parameter\n# KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, \n# metric=’minkowski’, metric_params=None, n_jobs=1, **kwargs)\n\n# methods of\n# fit(X, y) : Fit the model using X as training data and y as target values\n# predict(X):Predict the class labels for the provided data\n# predict_proba(X):Return probability estimates for the test data X.\n#-------------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n#-------------------------------------\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample query point 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 1\npredicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.2.4. Sample Query Point-2 </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 100\n\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4.3. Logistic Regression</h2>"},{"metadata":{},"cell_type":"markdown","source":"4.3.1. With Class balancing"},{"metadata":{},"cell_type":"markdown","source":"<h4>4.3.1.1. Hyper paramter tuning</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>4.3.1.2. Testing the model with best hyper paramters</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y, cv_x_tfidf, cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_imp_feature_names(text, indices, removed_ind = []):\n    word_present = 0\n    tabulte_list = []\n    incresingorder_ind = 0\n    for i in indices:\n        if i < train_gene_feature_tfidf.shape[1]:\n            tabulte_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n        elif i< 18:\n            tabulte_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n        if ((i > 17) & (i not in removed_ind)) :\n            word = train_text_features[i]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n            tabulte_list.append([incresingorder_ind,train_text_features[i], yes_no])\n        incresingorder_ind += 1\n    print(word_present, \"most importent features are present in our query point\")\n    print(\"-\"*50)\n    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n    print (tabulate(tabulte_list, headers=[\"Index\",'Feature name', 'Present or Not']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5>4.3.1.3.1. Correctly Classified point</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Incorrectly classified point"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.3.2. Without Class balancing</h3>\n"},{"metadata":{},"cell_type":"markdown","source":"<h4>4.3.2.1. Hyper paramter tuning</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\n\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>4.3.2.2. Testing model with best hyper parameters</h4>\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y, cv_x_tfidf, cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4.4. Linear Support Vector Machines</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.4.1. Hyper paramter tuning</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n# --------------------------------\n\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-5, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.4.2. Testing model with best hyper parameters</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n# --------------------------------\n\n\n# clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y,cv_x_tfidf,cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.3.3. Feature Importance</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_tfidf,train_y)\ntest_point_index = 1\n# test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4.5 Random Forest Classifier</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.1. Hyper paramter tuning (With One hot Encoding)</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_tfidf, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_tfidf, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\n'''fig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.2. Testing model with best hyper parameters (TF - IDF)</h3>"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y,cv_x_tfidf,cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.3. Feature Importance</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_point_index = 10\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.3. Hyper paramter tuning (With Response Coding)</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n'''\nfig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/4)],max_depth[int(i%4)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.4. Testing model with best hyper parameters (Response Coding)</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\nclf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.5.5. Feature Importance</h3>"},{"metadata":{},"cell_type":"markdown","source":"#### query point 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\n\ntest_point_index = 1\nno_feature = 27\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### query point 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 100\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>4.7 Stack the models </h2>\n"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.7.1 testing with hyper parameter tuning</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\n\n\n# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n# --------------------------------\n\n\n# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n# --------------------------------\n\n\nclf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf1.fit(train_x_tfidf, train_y)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf2.fit(train_x_tfidf, train_y)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(train_x_tfidf, train_y)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\nsig_clf1.fit(train_x_tfidf, train_y)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf1.predict_proba(cv_x_tfidf))))\nsig_clf2.fit(train_x_tfidf, train_y)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf2.predict_proba(cv_x_tfidf))))\nsig_clf3.fit(train_x_tfidf, train_y)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf3.predict_proba(cv_x_tfidf))))\nprint(\"-\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(train_x_tfidf, train_y)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(cv_y, sclf.predict_proba(cv_x_tfidf))))\n    log_error =log_loss(cv_y, sclf.predict_proba(cv_x_tfidf))\n    if best_alpha > log_error:\n        best_alpha = log_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.7.2 testing the model with the best hyper parameters</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(train_x_tfidf, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_tfidf))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(cv_y, sclf.predict_proba(cv_x_tfidf))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_tfidf))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_tfidf)- test_y))/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=sclf.predict(test_x_tfidf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.7.3 Maximum Voting classifier </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Refer:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\nfrom sklearn.ensemble import VotingClassifier\nvclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\nvclf.fit(train_x_tfidf, train_y)\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_tfidf)))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_tfidf)))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_tfidf)))\nprint(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_tfidf)- test_y))/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=vclf.predict(test_x_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 3 "},{"metadata":{},"cell_type":"markdown","source":"# Logistics Regression with Unigram and Bigram"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = TfidfVectorizer(min_df=3 ,ngram_range = (1,2) )\ntrain_text_feature_tfidf = text_vectorizer.fit_transform(train_df['TEXT'])\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_tfidf.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_tfidf.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# don't forget to normalize every feature\ntrain_text_feature_tfidf = normalize(train_text_feature_tfidf, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_tfidf = text_vectorizer.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_tfidf = normalize(test_text_feature_tfidf, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_tfidf = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\ncv_text_feature_tfidf = normalize(cv_text_feature_tfidf, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging gene, variance and text features\n\n# building train, test and cross validation data sets\n# a = [[1, 2], \n#      [3, 4]]\n# b = [[4, 5], \n#      [6, 7]]\n# hstack(a, b) = [[1, 2, 4, 5],\n#                [ 3, 4, 6, 7]]\n\ntrain_gene_var_onehotCoding = hstack((train_gene_ohe,train_variation_ohe))\ntest_gene_var_onehotCoding = hstack((test_gene_ohe,test_variation_ohe))\ncv_gene_var_onehotCoding = hstack((cv_gene_ohe,cv_variation_ohe))\n\ntrain_x_tfidf = hstack((train_gene_var_onehotCoding, train_text_feature_tfidf)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_tfidf = hstack((test_gene_var_onehotCoding, test_text_feature_tfidf)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_tfidf = hstack((cv_gene_var_onehotCoding, cv_text_feature_tfidf)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n\n\ntrain_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"tfidf features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_tfidf.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_tfidf.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h2> Logistic Regression</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3> With Class balancing</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h4> Hyper paramter tuning</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Testing the model with best hyper paramters</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y, cv_x_tfidf, cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Feature Importance</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_imp_feature_names(text, indices, removed_ind = []):\n    word_present = 0\n    tabulte_list = []\n    incresingorder_ind = 0\n    for i in indices:\n        if i < train_gene_feature_tfidf.shape[1]:\n            tabulte_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n        elif i< 18:\n            tabulte_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n        if ((i > 17) & (i not in removed_ind)) :\n            word = train_text_features[i]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n            tabulte_list.append([incresingorder_ind,train_text_features[i], yes_no])\n        incresingorder_ind += 1\n    print(word_present, \"most importent features are present in our query point\")\n    print(\"-\"*50)\n    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n    print (tabulate(tabulte_list, headers=[\"Index\",'Feature name', 'Present or Not']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5> Query point 1</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5> Query point 2</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Without Class balancing</h3>\n"},{"metadata":{},"cell_type":"markdown","source":"<h4> Hyper paramter tuning</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n#------------------------------\n\n\n\n# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_tfidf, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_tfidf, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_tfidf)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_tfidf, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_tfidf)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Testing model with best hyper parameters</h4>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_tfidf, train_y, cv_x_tfidf, cv_y, clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Feature Importance, Query point 1t</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_tfidf,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Feature Importance, Query  point 2</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}